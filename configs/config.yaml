experiment: turbines_experiment
datasets:
  mnist:
    name: mnist
    input_dim: 784
    output_dim: 10
    urls:
      train_images: https://raw.githubusercontent.com/fgnt/mnist/master/train-images-idx3-ubyte.gz
      train_labels: https://raw.githubusercontent.com/fgnt/mnist/master/train-labels-idx1-ubyte.gz
      test_images: https://raw.githubusercontent.com/fgnt/mnist/master/t10k-images-idx3-ubyte.gz
      test_labels: https://raw.githubusercontent.com/fgnt/mnist/master/t10k-labels-idx1-ubyte.gz
    paths:
      raw_dir: "data/raw_mnist"
      processed_dir: "data/processed/mnist"
      marker_file: "mnist_ready.success"
    train_config:
      loss_fn: CCE
      metric: f1
    split:
      train: 0.9
      val: 0.1
      test: 0
  cancer:
    name: cancer
    input_dim: 30
    output_dim: 1
    paths:
      processed_dir: "data/processed/cancer"
      marker_file: "cancer_ready.success"     
    split:
      train: 0.8
      val: 0.1
      test: 0.1 
    train_config:
      loss_fn: BCE
      metric: f1
  turbines:
    name: turbines
    input_dim: 4
    output_dim: 1
    paths:
      processed_dir: "data/processed/turbines"
      marker_file: "cancer_ready.success"
    split:
      train: 0.8
      val: 0.1
      test: 0.1
    train_config:
      loss_fn: MAE
      metric: r2

model:
  tiny:
    type: sequential
    architecture:
      - [null, 64, relu]
      #- [batchnorm, 64]
      - [Dropout, 0.2]
      - [64, null, linear]
      # (input,64) -> relu -> (64, 1) -> linear
      # (input,64, output) ----> 1 layer
  medium:
    type: sequential
    architecture:
      - [null, 64, relu]
      - [64, 128, relu]
      #- [batchnorm, 128]
      - [Dropout, 0.2]
      - [128, 128, relu]
      - [Dropout, 0.2]
      - [128, null, linear]
      # (input,64) -> relu -> (64, 128) -> relu -> (128, 1) -> linear
      # (input, 64, 128, 128, output) ----> 3 layers
  large:
    type: sequential
    architecture:
      - [null, 64, relu]
      - [batchnorm, 64]
      - [Dropout, 0.2]
      - [64, 128, relu]
      # - [batchnorm, 128]
      - [Dropout, 0.2]
      - [128, 256, relu]
      # - [batchnorm, 256]
      - [Dropout, 0.2]
      - [256, 128, relu]
      # - [batchnorm, 128]
      - [Dropout, 0.2]
      - [128, 64, relu]
      - [64, null, linear]
      # (input, 64, 128, 256, 128, 64, output)  ---> 5 layers

training:
  batch_size: 64
  epochs: 100
  optimizer:
    name: adamw
    lr: 0.001
  early_stopping: True
  patience: 5
  min_delata: 0.001
